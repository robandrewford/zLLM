#!/usr/bin/env python3
"""
Complete xLLM Workflow

This script demonstrates the complete workflow for xLLM:
1. Process PDFs and scraped content
2. Combine the processed data
3. Generate backend tables
4. Process queries using the backend tables
5. Generate output files

This ensures that all the .txt files generated by xllm6 are also generated by xLLM.
"""

import argparse
import logging
import os
import sys
import time
from pathlib import Path
import json
import shutil

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("xllm_workflow.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("xllm_workflow")

# Sample test queries to run against the knowledge base
TEST_QUERIES = [
    "machine learning",
    "neural networks",
    "probability theory",
    "statistics",
    "deep learning",
    "artificial intelligence",
    "data science",
    "natural language processing",
    "computer vision",
    "reinforcement learning",
]

def process_data_sources(pdf_dir, scrape_dir, processed_dir):
    """
    Process PDFs and scraped content.

    Args:
        pdf_dir: Directory containing PDF files
        scrape_dir: Directory containing scraped content
        processed_dir: Directory to save processed data

    Returns:
        Tuple of (pdf_data_files, scraped_data_files)
    """
    logger.info("Step 1: Processing data sources")

    # Create processed directories
    pdf_processed_dir = os.path.join(processed_dir, "pdfs")
    scraped_processed_dir = os.path.join(processed_dir, "scraped")
    os.makedirs(pdf_processed_dir, exist_ok=True)
    os.makedirs(scraped_processed_dir, exist_ok=True)

    # Process PDFs
    pdf_data_files = []
    if os.path.exists(pdf_dir):
        logger.info(f"Processing PDFs from {pdf_dir}")
        try:
            from xllm.processors import PDFProcessor
            processor = PDFProcessor(output_dir=pdf_processed_dir)

            for pdf_file in os.listdir(pdf_dir):
                if pdf_file.lower().endswith('.pdf'):
                    pdf_path = os.path.join(pdf_dir, pdf_file)
                    output_file = os.path.join(pdf_processed_dir, f"{os.path.splitext(pdf_file)[0]}_processed.json")

                    logger.info(f"Processing PDF: {pdf_path}")
                    result = processor.process_file(pdf_path)

                    with open(output_file, "w", encoding="utf-8") as f:
                        json.dump(result, f, indent=2, default=lambda x: list(x) if isinstance(x, set) else x)

                    pdf_data_files.append(output_file)
        except ImportError:
            logger.warning("PDFProcessor not available. Skipping PDF processing.")
    else:
        logger.warning(f"PDF directory {pdf_dir} not found. Skipping PDF processing.")

    # Process scraped content
    scraped_data_files = []
    if os.path.exists(scrape_dir):
        logger.info(f"Processing scraped content from {scrape_dir}")
        try:
            from xllm.processors import WebContentProcessor
            processor = WebContentProcessor(output_dir=scraped_processed_dir)

            for scraped_file in os.listdir(scrape_dir):
                if scraped_file.lower().endswith(('.txt', '.json', '.html')):
                    scraped_path = os.path.join(scrape_dir, scraped_file)
                    output_file = os.path.join(scraped_processed_dir, f"{os.path.splitext(scraped_file)[0]}_processed.json")

                    logger.info(f"Processing scraped content: {scraped_path}")
                    result = processor.process_file(scraped_path)

                    with open(output_file, "w", encoding="utf-8") as f:
                        json.dump(result, f, indent=2, default=lambda x: list(x) if isinstance(x, set) else x)

                    scraped_data_files.append(output_file)
        except ImportError:
            logger.warning("WebContentProcessor not available. Skipping scraped content processing.")
    else:
        logger.warning(f"Scrape directory {scrape_dir} not found. Skipping scraped content processing.")

    logger.info(f"Processed {len(pdf_data_files)} PDF files and {len(scraped_data_files)} scraped content files")
    return pdf_data_files, scraped_data_files

def combine_data(pdf_data_files, scraped_data_files, combined_dir):
    """
    Combine processed data from PDFs and scraped content.

    Args:
        pdf_data_files: List of processed PDF data files
        scraped_data_files: List of processed scraped data files
        combined_dir: Directory to save combined data

    Returns:
        Path to the combined data file
    """
    logger.info("Step 2: Combining processed data")

    # Create combined directory
    os.makedirs(combined_dir, exist_ok=True)

    # Output file for combined data
    combined_file = os.path.join(combined_dir, f"combined_data_{int(time.time())}.json")

    combined_data = []

    # Process PDF data files
    for pdf_file in pdf_data_files:
        try:
            with open(pdf_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                data["source_type"] = "pdf"
                data["source_file"] = os.path.basename(pdf_file)
                combined_data.append(data)
        except Exception as e:
            logger.error(f"Error processing PDF data file {pdf_file}: {e}")

    # Process scraped data files
    for scraped_file in scraped_data_files:
        try:
            with open(scraped_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                data["source_type"] = "web"
                data["source_file"] = os.path.basename(scraped_file)
                combined_data.append(data)
        except Exception as e:
            logger.error(f"Error processing scraped data file {scraped_file}: {e}")

    # Save combined data
    with open(combined_file, "w", encoding="utf-8") as f:
        json.dump(combined_data, f, indent=2)

    logger.info(f"Combined data saved to {combined_file}")
    logger.info(f"Combined {len(combined_data)} data entries")

    return combined_file

def generate_backend_tables(combined_data_file, tables_dir, use_existing=False):
    """
    Generate backend tables from combined data.

    Args:
        combined_data_file: Path to the combined data file
        tables_dir: Directory to save backend tables
        use_existing: Whether to use existing tables if available

    Returns:
        Dictionary mapping table names to file paths
    """
    logger.info("Step 3: Generating backend tables")

    # Check if we should use existing tables
    if use_existing and os.path.exists(tables_dir) and os.listdir(tables_dir):
        logger.info(f"Using existing backend tables in {tables_dir}")

        # Get the list of existing tables
        tables = {}
        for table_name in [
            "dictionary", "embeddings", "word_hash", "hash_see", "hash_related",
            "hash_category", "ngrams_table", "compressed_ngrams_table",
            "compressed_word2_hash", "word2_pairs", "url_map", "arr_url"
        ]:
            table_path = os.path.join(tables_dir, f"{table_name}.txt")
            if os.path.exists(table_path):
                tables[table_name] = table_path
                logger.info(f"Found existing table: {table_name}")
            else:
                logger.warning(f"Missing table: {table_name}")

        return tables

    # Create tables directory
    os.makedirs(tables_dir, exist_ok=True)

    try:
        # Import the knowledge base builder
        from xllm.knowledge_base import HashKnowledgeBase

        # Initialize the knowledge base
        kb = HashKnowledgeBase(output_dir=Path(tables_dir))

        # Load the combined data
        with open(combined_data_file, "r", encoding="utf-8") as f:
            combined_data = json.load(f)

        # Process each data entry
        for entry in combined_data:
            kb.add_data(entry)

        # Build the derived tables
        kb.build_derived_tables()

        # Save the knowledge base
        kb.save(Path(tables_dir))

        # Get the list of generated tables
        tables = {}
        for table_name in [
            "dictionary", "embeddings", "word_hash", "hash_see", "hash_related",
            "hash_category", "ngrams_table", "compressed_ngrams_table",
            "compressed_word2_hash", "word2_pairs", "url_map", "arr_url"
        ]:
            table_path = os.path.join(tables_dir, f"{table_name}.txt")
            if os.path.exists(table_path):
                tables[table_name] = table_path
                logger.info(f"Generated table: {table_name}")
            else:
                logger.warning(f"Failed to generate table: {table_name}")

        logger.info(f"Backend tables generated in {tables_dir}")
        return tables

    except ImportError:
        logger.error("HashKnowledgeBase not available. Cannot generate backend tables.")
        return {}

def process_queries(tables_dir, queries, output_dir):
    """
    Process queries using the backend tables and generate output files.

    Args:
        tables_dir: Directory containing backend tables
        queries: List of queries to process
        output_dir: Directory to save output files

    Returns:
        List of output file paths
    """
    logger.info("Step 4: Processing queries")

    # Create output directory
    os.makedirs(output_dir, exist_ok=True)

    output_files = []

    try:
        # Import the query engine
        from xllm.query_engine import KnowledgeQueryEngine
        from xllm.knowledge_base import HashKnowledgeBase

        # Load the knowledge base
        kb = HashKnowledgeBase(output_dir=Path(tables_dir))
        kb.load(Path(tables_dir))  # Pass the path to the load method

        # Initialize the query engine
        query_engine = KnowledgeQueryEngine(kb)

        # Process each query
        for query in queries:
            logger.info(f"Processing query: {query}")

            # Generate output file path
            output_file = os.path.join(output_dir, f"xllm_{query.replace(' ', '_')}.txt")

            # Process the query
            results = query_engine.query(query)

            # Format the results
            formatted_results = query_engine.format_results(results)

            # Save the results to the output file
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(f"Query: {query}\n\n")
                f.write(formatted_results)

            logger.info(f"Results saved to {output_file}")
            output_files.append(output_file)

        logger.info(f"Processed {len(queries)} queries")
        return output_files

    except ImportError:
        logger.error("Query engine not available. Cannot process queries.")
        return []

def compare_with_xllm6(xllm_output_dir, xllm6_output_dir):
    """
    Compare xLLM output files with xllm6 output files.

    Args:
        xllm_output_dir: Directory containing xLLM output files
        xllm6_output_dir: Directory containing xllm6 output files

    Returns:
        Dictionary mapping query names to comparison results
    """
    logger.info("Step 5: Comparing with xllm6 outputs")

    import difflib

    # Check if xllm6 output directory exists
    if not os.path.exists(xllm6_output_dir):
        logger.warning(f"xllm6 output directory {xllm6_output_dir} not found. Skipping comparison.")
        return {}

    # Get xLLM output files
    xllm_files = [f for f in os.listdir(xllm_output_dir) if f.startswith("xllm_") and f.endswith(".txt")]

    # Get xllm6 output files
    xllm6_files = [f for f in os.listdir(xllm6_output_dir) if f.startswith("xllm6_") and f.endswith(".txt")]

    # Map xLLM files to xllm6 files
    comparison_results = {}

    for xllm_file in xllm_files:
        # Extract query name from file name
        query_name = xllm_file[5:-4]  # Remove "xllm_" prefix and ".txt" suffix

        # Find corresponding xllm6 file
        xllm6_file = f"xllm6_{query_name}.txt"

        if xllm6_file in xllm6_files:
            # Compare the files
            with open(os.path.join(xllm_output_dir, xllm_file), "r", encoding="utf-8") as f1, \
                 open(os.path.join(xllm6_output_dir, xllm6_file), "r", encoding="utf-8") as f2:
                xllm_content = f1.readlines()
                xllm6_content = f2.readlines()

            # Calculate the difference
            diff = list(difflib.unified_diff(
                xllm6_content, xllm_content,
                fromfile=xllm6_file, tofile=xllm_file,
                lineterm=""
            ))

            # Store the comparison result
            if diff:
                comparison_results[query_name] = {
                    "identical": False,
                    "diff": "\n".join(diff)
                }
            else:
                comparison_results[query_name] = {
                    "identical": True,
                    "diff": ""
                }

            logger.info(f"Compared {xllm_file} with {xllm6_file}: {'identical' if not diff else 'different'}")
        else:
            logger.warning(f"No corresponding xllm6 file found for {xllm_file}")
            comparison_results[query_name] = {
                "identical": False,
                "diff": "No corresponding xllm6 file found"
            }

    # Check for xllm6 files without corresponding xLLM files
    for xllm6_file in xllm6_files:
        query_name = xllm6_file[6:-4]  # Remove "xllm6_" prefix and ".txt" suffix
        xllm_file = f"xllm_{query_name}.txt"

        if xllm_file not in xllm_files:
            logger.warning(f"No corresponding xLLM file found for {xllm6_file}")
            comparison_results[query_name] = {
                "identical": False,
                "diff": "No corresponding xLLM file found"
            }

    # Generate comparison report
    identical_count = sum(1 for result in comparison_results.values() if result["identical"])
    different_count = len(comparison_results) - identical_count

    logger.info(f"Comparison complete: {identical_count} identical, {different_count} different")

    return comparison_results

def main():
    """Main function to run the complete xLLM workflow."""
    parser = argparse.ArgumentParser(description="Complete xLLM Workflow")
    parser.add_argument("--pdf-dir", type=str, default="data/pdfs",
                        help="Directory containing PDF files")
    parser.add_argument("--scrape-dir", type=str, default="data/scraped",
                        help="Directory containing scraped content")
    parser.add_argument("--processed-dir", type=str, default="data/processed",
                        help="Directory to save processed data")
    parser.add_argument("--combined-dir", type=str, default="data/combined",
                        help="Directory to save combined data")
    parser.add_argument("--tables-dir", type=str, default="xLLM/data/knowledge",
                        help="Directory to save backend tables")
    parser.add_argument("--output-dir", type=str, default="data/output",
                        help="Directory to save query output files")
    parser.add_argument("--xllm6-output-dir", type=str, default="xllm6_output",
                        help="Directory containing xllm6 output files for comparison")
    parser.add_argument("--use-existing-tables", action="store_true",
                        help="Use existing backend tables if available")
    parser.add_argument("--skip-data-processing", action="store_true",
                        help="Skip data processing and go straight to query processing")
    parser.add_argument("--skip-comparison", action="store_true",
                        help="Skip comparison with xllm6 outputs")
    parser.add_argument("--queries", type=str, nargs="+",
                        help="Custom queries to process (default: use predefined test queries)")
    args = parser.parse_args()

    # Track start time for performance measurement
    start_time = time.time()

    # Use custom queries if provided, otherwise use test queries
    queries = args.queries if args.queries else TEST_QUERIES

    if not args.skip_data_processing:
        # Step 1: Process data sources
        pdf_data_files, scraped_data_files = process_data_sources(
            args.pdf_dir, args.scrape_dir, args.processed_dir
        )

        # Step 2: Combine data
        combined_file = combine_data(
            pdf_data_files, scraped_data_files, args.combined_dir
        )

        # Step 3: Generate backend tables
        tables = generate_backend_tables(
            combined_file, args.tables_dir, args.use_existing_tables
        )
    else:
        logger.info("Skipping data processing steps")
        tables = {}  # We'll use existing tables

    # Step 4: Process queries
    output_files = process_queries(
        args.tables_dir, queries, args.output_dir
    )

    # Step 5: Compare with xllm6 outputs (if requested)
    if not args.skip_comparison:
        comparison_results = compare_with_xllm6(
            args.output_dir, args.xllm6_output_dir
        )

        # Generate comparison report
        report_path = os.path.join(args.output_dir, "comparison_report.txt")
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("# xLLM vs xllm6 Comparison Report\n\n")

            identical_count = sum(1 for result in comparison_results.values() if result["identical"])
            different_count = len(comparison_results) - identical_count

            f.write(f"Total queries: {len(comparison_results)}\n")
            f.write(f"Identical results: {identical_count}\n")
            f.write(f"Different results: {different_count}\n\n")

            for query_name, result in comparison_results.items():
                status = "✅ IDENTICAL" if result["identical"] else "❌ DIFFERENT"
                f.write(f"## Query: {query_name.replace('_', ' ')} - {status}\n\n")

                if not result["identical"]:
                    f.write("```diff\n")
                    f.write(result["diff"])
                    f.write("\n```\n\n")

        logger.info(f"Comparison report saved to {report_path}")

    # Calculate and log total processing time
    total_time = time.time() - start_time
    logger.info(f"Total processing time: {total_time:.2f} seconds")

    # Print summary
    print("\nComplete xLLM Workflow Summary:")
    if not args.skip_data_processing:
        print(f"PDFs processed: {len(pdf_data_files)}")
        print(f"Scraped content files processed: {len(scraped_data_files)}")
        print(f"Backend tables directory: {args.tables_dir}")
    print(f"Queries processed: {len(output_files)}")
    print(f"Output files directory: {args.output_dir}")
    if not args.skip_comparison:
        identical_count = sum(1 for result in comparison_results.values() if result["identical"])
        different_count = len(comparison_results) - identical_count
        print(f"Comparison results: {identical_count} identical, {different_count} different")
        print(f"Comparison report: {report_path}")
    print(f"Total processing time: {total_time:.2f} seconds")

    return 0

if __name__ == "__main__":
    sys.exit(main())
